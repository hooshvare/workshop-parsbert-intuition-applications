{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMx1-gRpPPsm",
        "cellView": "form",
        "outputId": "31172e32-81ad-4775-9b9c-8ae1185883e5"
      },
      "source": [
        "#@title Install Required Packages\n",
        "\n",
        "# On your local machine, uncomment them\n",
        "# !pip install -qU torch\n",
        "# !pip install -qU numpy\n",
        "# !pip install -qU pandas\n",
        "\n",
        "!pip install -qU transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8MB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 25.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 44.8MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kgRfn5FTGrF",
        "cellView": "form"
      },
      "source": [
        "#@title Load Packages\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "from pprint import pprint\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jSFpjrFcKRC"
      },
      "source": [
        "# Word Embedding\n",
        "\n",
        "The way machine learning models see data is different from how we (humans) do. For example, we can easily understand the text, I saw a cat, but our models cannot.\n",
        "They need vectors of features. Such vectors, or word embeddings, are representations of words which can be fed into your model.\n",
        "\n",
        "<br/>\n",
        "\n",
        "In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance. For each vocabulary word, a look-up table contains its embedding. This embedding can be found the word index in the vocabulary.\n",
        "\n",
        "<br/>\n",
        "\n",
        "To account for unknown words (the ones which are not in the vocabulary), usually a vocabulary contains a special token UNK. Alternatively, unknown tokens can be ignored or assigned a zero vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm96bijOIn3M"
      },
      "source": [
        "# How Do We Get These Word Vectors?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEKTn9qgItgM"
      },
      "source": [
        "## Represent as Discrete Symbols: One-hot Vectors\n",
        "\n",
        "The easiest you can do is to represent words as one-hot vectors: for the i-th word in the vocabulary, the vector has 1 on the i-th dimension and 0 on the rest. In Machine Learning, this is the most simple way to represent categorical features.\n",
        "\n",
        "One of the problems is that for large vocabularies, these vectors will be very long: vector dimensionality is equal to the vocabulary size.\n",
        "What is really important, is that these vectors know nothing about the words they represent.\n",
        "\n",
        "<br/>\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://hooshvare.s3.ir-thr-at1.arvanstorage.com/one-hot.png\" />\n",
        "    <br/>\n",
        "    <em>Figure 1: One Hot Encoding</em>\n",
        "</p>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pup1NYFnIv3J"
      },
      "source": [
        "## Distributional Semantics\n",
        "\n",
        "Words which frequently appear in similar contexts have similar meaning.\n",
        "Main idea: We need to put information about word contexts into word representation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNfEYVSBI0B5"
      },
      "source": [
        "## Count-Based Methods\n",
        "\n",
        "Put this information manually based on global corpus statistics.\n",
        "\n",
        "- Co-Occurence Counts\n",
        "- Positive Pointwise Mutual Information (PPMI)\n",
        "- Latent Semantic Analysis (LSA): Understanding Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfGHbBs8I3qj"
      },
      "source": [
        "## Word2Vec: a Prediction-Based Method\n",
        "\n",
        "Learn word vectors by teaching them to predict contexts.\n",
        "Word2Vec is a model whose parameters are word vectors. These parameters are optimized iteratively for a certain objective. The objective forces word vectors to \"know\" contexts a word can appear in: the vectors are trained to predict possible contexts of the corresponding words.\n",
        "\n",
        "Word2Vec is an iterative method. Its main idea is as follows:\n",
        "\n",
        "\n",
        "- Take a huge text corpus\n",
        "- Go over the text with a sliding window, moving one word at a time. At each step, there is a central word and context words (other words in this window)\n",
        "- For the central word, compute probabilities of context words\n",
        "- Adjust the vectors to increase these probabilities\n",
        "\n",
        "\n",
        "Word2Vec variants: Skip-Gram and CBOW\n",
        "\n",
        "- **Skip-Gram**: it predicts context words given the central word. Skip-Gram with negative sampling is the most popular approach.\n",
        "- **CBOW** (Continuous Bag-of-Words) predicts the central word from the sum of context vectors. This simple sum of word vectors is called \"bag of words\", which gives the name for the model.\n",
        "\n",
        "<br/>\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://hooshvare.s3.ir-thr-at1.arvanstorage.com/w2v.jpg\" />\n",
        "    <br/>\n",
        "    <em>Figure 2: Word2Vec</em>\n",
        "</p>\n",
        "<br/>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou6_fgGVJHpb"
      },
      "source": [
        "## GloVe: Global Vectors for Word Representation\n",
        "\n",
        "The GloVe model is a combination of count-based methods and prediction methods (e.g., Word2Vec). Model name, GloVe, stands for \"Global Vectors\", which reflects its idea: the method uses global information from corpus to learn vectors.\n",
        "\n",
        "The simplest count-based method uses co-occurrence counts to measure the association between word $w$ and context $c: N(w, c)$. GloVe also uses these counts to construct the loss function\n",
        "\n",
        "\n",
        "Similar to Word2Vec, we also have different vectors for central and context words - these are our parameters. Additionally, the method has a scalar bias term for each word vector.\n",
        "\n",
        "\n",
        "What is especially interesting, is the way GloVe controls the influence of rare and frequent words: loss for each pair $(w, c)$ is weighted in a way that\n",
        "\n",
        "- rare events are penalized\n",
        "- very frequent events are not over-weighted\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxepoErzJSQp"
      },
      "source": [
        "## FastText\n",
        "\n",
        "In 2016 Facebook research team proposed a method and released a library for both learning word representation and sentence classification. \n",
        "\n",
        "\n",
        "FastText differs in the sense that other word representation methods such as skip-gram, CBOW and Glove treat every single word as a smallest unit whose vector representation is to be found. However, FastText assumes a word to be formed by an n-grams of characters, for examlpe, sunny is composed of $[sun, sunn, sunny], [sunny, unny, nny]$ etc, where n could range from 1 to the range of the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbsyEEt5JX5B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}